\documentclass[conference]{IEEEtran}
\usepackage{pdftexcmds}
\usepackage[pdftex]{graphicx}
\usepackage{multirow}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{balance}
\usepackage{amssymb, marvosym}
\usepackage{threeparttable}
\usepackage[bookmarks=false]{hyperref}
\usepackage{url}
\PassOptionsToPackage{hyphens}{url}
\hypersetup{colorlinks=true,breaklinks=true}
\usetikzlibrary{patterns,shapes,arrows}
\newcommand\tab[1][.5cm]{\hspace*{#1}}
\hyphenation{op-tical net-works semi-conduc-tor}
\raggedbottom
\newcommand{\tool}{\textsl{tool-recommender-bot}}
\newcommand{\pseudosection}[1]{\vspace{2mm} \noindent {\bf #1}}
\newcommand{\pseudosubsection}[1]{\vspace{2mm} \noindent{\it #1}}

\begin{document}

% Copyright
%\setcopyright{acmlicensed}

\title{\tool}

\author{\IEEEauthorblockN{Chris Brown and Emerson Murphy-Hill}
\IEEEauthorblockA{Department of Computer Science\\
North Carolina State University\\
Raleigh, NC\\
Email: dcbrow10@ncsu.edu, emerson@csc.ncsu.edu}
}

\maketitle
\begin{abstract}
Software developers have access to numerous tools designed to increase productivity by automatically performing development tasks, but these tools are often ignored or undiscovered by programmers. This lack of tool awareness can have dangerous consequences, especially in the software engineering industry. Automated recommendation systems were created to improve tool discovery, however these systems are not as effective as user-to-user recommendations. To increase awareness of helpful development tools, we introduce \textit{\tool}, a novel system that integrates concepts of peer interactions and software engineering practices to increase tool discovery by effectively making recommendations to developers on open source GitHub projects.
\end{abstract}

\begin{IEEEkeywords}
Software Engineering; Tool Recommendation; Tool Discovery; Open Source
\end{IEEEkeywords}

\section{Introduction}

Software is becoming more and more prevalent throughout our society. To keep up with the increasing demands for technology, software engineers are making efforts to emphasize \textbf{---}, or \textit{software quality}. Throughout the software development life cycle, more focus is being put on metrics that impact both software producers and consumers such as functionality, reliability, usability, efficiency, maintainability, and portability~\cite{KitchenhamQualityTarget}. However, despite increased attention to software quality, defects in code remains a persisting and escalating problem. 

The 2017 Software Fail Watch by Tricentis, a software testing company, discovered that software failures impacted approximaltely half of the world's population (3.7 billion users) and caused \$1.7 trillion in financial losses.\footnote{https://www.tricentis.com/software-fail-watch/} Additionally, the process of finding and fixing bugs in code, or debugging, is becoming increasingly time-consuming and costly.  A study by the National Institute of Standards and Technology reported that software engineers spend 70-80\% of their time debugging at work, and one error takes an average of 17.4 hours to debug~\cite{NIST}. Studies also show the cost of fixing an error becomes more expensive the longer a bug exists in source code~\cite{SEEconomics, SoftwareAssuranceSDLC}.

To improve code quality and prevent errors, researchers and toolsmiths have created numerous software engineering tools to aid developers in their work. These tools automatically perform a wide variety of software development tasks to save time and effort for programmers. Research shows that tools for static analysis~\cite{UsingStaticAnalysis}, refactoring~\cite{Murphy-HillFitness}, security, code navigation, and more are beneficial for preventing errors and improving software quality. Ultimately, these useful tools can lead to reduced debugging costs and increased developer productivity.

Althouth quality is a primary concern for software engineers and users, developers rarely use tools designed to improve code~\cite{Johnson2013Why}. There are many barriers to tool adoption, and one of the primary reasons useful tools are underused or ignored is the discoverability barrier, or when users are unaware of a tool's existence~\cite{Murphy-HillScreencastingDiscovery}. This lack of awareness for software engineering tools can lead to poor code quality, harmed users, and significant amounts of time and money spent fixing errors. While many automated approaches have been developed to increase knowledge of useful software tools and features, Murphy-Hill and colleagues found that learning about tools from colleagues during normal work activities, or \textit{peer interaction}, is the most effective mode of tool discovery~\cite{MurphyHill2011PeerInteraction}.

To help solve the tool discovery problem among software engineers, we developed \tool. Our system automatically recommends development tools by integrating characteristics of peer interactions and software engineering practices. \tool~is designed for toolsmiths to increase awareness of their products by systematically running their tools and making customized recommendations to developers on GitHub\footnote{https://github.com}, the largest code hosting platform in the world~\cite{GousiosGitHub}. This paper seeks to answer the following research questions (RQs): \\

\noindent
\textbf{RQ1:} How applicable is \tool~to real-world software applications?  \\ 
\textbf{RQ2:} How useful are recommendations from \tool~to developers?  \\

To answer these questions, we evaluated \tool~by recommending \textsc{Error Prone}\footnote{http://errorprone.info}, an open source Java static analysis tool. We examined how often recommendations were made and how developers reacted to receiving suggestions from our system. This research makes the following contributions:

\begin{itemize}
 \item a novel system for researchers to recommend software engineering tools on GitHub, and
 \item an evaluation on the effectiveness of \tool~with \textsc{Error Prone}
 \end{itemize}

\section{Related Work}

Our implementation of \tool~builds on previous research examining tool discovery, lack of tool adoption among software engineers, and automated recommendation systems.

Researchers have explored how humans learn about new tools. There are various methods to discover tools in software, and research suggests recommendations between peers is the most effective way to increase tool awareness. Murphy-Hill found that peer interactions were the most effective mode of tool discovery compared to tool encounters, tutorials, descriptions, social media, and discussion threads~\cite{MurphyHill2011PeerInteraction}~\cite{Murphy-Hill2015HowDoUsers}. Similarly, Welty discovered that software users sought help from colleagues more often than search engines and help menus~\cite{Welty2011Help}. To improve the effectiveness of recommendations from our system, we integrate qualities of peer interactions into \tool.

Previous work has also explored the tool discovery problem and barriers preventing users from adopting new tools, specifically in the software engineering industry. Researchers have created numerous tools to aid software engineers in their work, but these products are often ignored by developers~\cite{Ivanov2017Gaps}. Tilley and colleagues studied the challenges of adopting these research-of-the-shelf tools in industry~\cite{Tilley2003ROTS}. Johnson and colleagues reported reasons why software engineers don't use static analysis tools to help find and prevent bugs in their code~\cite{Johnson2013Why}. Xiao and colleagues examined barriers and social influences blocking developers from using security tools to detect and prevent vulnerabilities and malicious attacks~\cite{Xiao2014Security}. Our project aims to increase tool discovery and adoption among developers by automatically recommending useful software enginering tools.

There are numerous existing technical approaches created to solve the tool discovery problem. Fischer and colleagues found that systems requiring users to explicitly seek help, or passive help systems, are ineffective and active help systems are more useful for increasing tool awareness~\cite{Fischer1984ActiveHelpSystems}. Gordon and colleagues developed Codepourri, a system using crowdsourcing to make recommendations for Python code~\cite{Gordon2015Codepourri}. Linton and colleagues designed a recommender system called OWL (organization-wide learning) to disseminate tool knowledge using logs throughout a company~\cite{Linton2000OWL}. ToolBox was developed as a ``community sensitive help system'' by Maltzahn to recommend Unix commands~\cite{Maltzahn1995Toolbox}. Answer Garden helps users discover new tools based on common questions asked by colleagues~\cite{Ackerman1990AnswerGarden}. SpyGlass automatically recommends tools to help users navigate code~\cite{Viriyakattiyaporn2010Spyglass}. We developed \tool~to actively suggest useful programming tools and increase awareness for software engineers. Our approach differs from existing recommendation systems in the design and implementation of our tool.

\section{Tool}
Our approach to improving tool discovery, \tool, aims to increase awareness and use of programming tools among software developers. This section describes the design and implementation of our system.

\subsection{Peer Interactions}
Previous research shows that recommendations between peers is an effective way to increase tool discovery and adoption~\cite{MurphyHill2011PeerInteraction}. Many existing help systems simulate user-to-user recommendations to increase awareness of application tools and features. 

To better understand what makes peer interactions an effective mode of tool discovery, our prior work observed how colleagues recommend tools to each other while working on tasks. Our results found that \emph{receptiveness} is a significant factor in determining the effectiveness of a tool recommendation, while other characteristics, such as politeness and persuasiveness, do not significantly impact the outcome~\cite{vlhcc17}. We designed \tool~to integrate user receptivity into our approach for making tool recommendations to increase awareness of programming tools.

\pseudosubsection{Receptiveness}

Previous work emphasizes the importance of receptivity. Fogg outlined best practicies for creating persuasive technology to change user behavior, and argued designers must choose a receptive audience~\cite{FoggPersuasive}. Our prior work defined receptiveness using two criteria introduced by Fogg: 1) demonstrating a desire and 2) familiarity with the target behavior and technology. Below we explain how \tool~was designed to recommend programming tools to software developers based on their desire and familiarity. \\

\subsubsection{Desire}

The primary desire of software users is to have enjoyable and problem-free experiences with software. Developers of these applications also have similar desires, to create high-quality and functioning programs for users. A 2002 study revealed that software engineers demonstrate this desire by spending the majority of the software development process and 70-80\% of their time testing and debugging code~\cite{NIST}. To aid developers in finding, fixing, and preventing various issues in code, many different types of tools have been created to help accomplish these tasks. However, despite the existence of effective tools for detecting errors, the number of bugs in software is increasing~\cite{HaveThingsChanged}. We aim to increase tool discovery by recommending software engineering tools designed to improve software quality. \\

\subsubsection{Familiarity}

Choosing an audience familiar with the target behavior is also vital to increasing adoption. To increase use of helpful programming tools, such as static analysis tools, our system focuses on making recommendations to software engineers within the context of the projects they develop. Familiarity with source code is important for creating software applications. Scalabrino and colleagues claim code understandability is one of the most important factors for software development, maintenance, debugging, and testing~\cite{Scalabrino2017Understandability}.

\subsection{Software Engineering}

\tool~incorporates key software engineering industry concepts to improve tool discovery among software developers.

\pseudosubsection{Continuous Integration}

Our system utilizes continuous integration to recommend useful tools before pull request changes are integrated into the main repository, or merged. \tool~is implemented as a plugin for Jenkins, ``the leading open source automation server'' for source code deployment and delivery.\footnote{https://jenkins.io/} The system uses Jenkins to clone Github repositories and periodically check for newly-opened pull requests every 15 minutes. When a new pull request is found, our system uses Jenkins to automatically run our approach to recommend useful software engineering tools.

To analyze the source code, we target projects that use the Maven~\footnote{https://maven.apache.org/} build automation and software management tool for Java applications. Our approach uses Maven to automatically handle dependencies and perform the static analysis when the project builds. We inject the desired software engineering tool as a Maven plugin to repository's \textit{pom.xml} project object model file to add it to the build process. \tool~then builds both the original version of the code before the proposed changes were made (base) and the changed version of the repository with the pull request modifications implemented (head) to inspect differences. Using Maven allows \tool~to run on a large number of Java projects that use the popular build tool and also makes our approach extendable to recommend other tools implemented as Maven plugins in future work. 

\pseudosubsection{Patch Identification}

After analyzing the base and head versions of the code, our approach parses the build output of each version to determine if any reported errors were fixed in the pull request. The software engineering tool identifies faults in the source code, and we take the tool's output and use it in an algorithm we developed to determine if pull request changes fix a reported bug. Our technique uses the code differencing tool GumTree~\cite{GumTree} to identify actions (addition, delete, insert, move, and update) performed between pull request versions and parse the code to convert the text into abstract syntax trees. 

To determine if an error was fixed, we take several things into consideration: First, our approach ignores instances where only delete actions were detected between the base and head versions of a file. This avoids making recommendations in situations where bugs were removed but not necessarily fixed in refactoring tasks, such as deleting and moving code, renaming classes, etc. Second, we ignore occurrences of deprecated classes because, similarly, the error reported was not fixed but removed. Third, we do not consider error fixes that were made by changes to a different file because we want to make recommendations where the developer is familiar with the changes that occurred. These help us minimize the number of false positives and errant recommendations in our approach.

%\pseudosubsection{Fix Localizaton}
%
%When a fix is identified in the pull request, \tool~then aims to find the location of the fix in the head version. To find the modified line that fixed a bug, we use GumTree to parse the Java file and convert it into abstract syntax trees. We look for the action closest to the offset of the error node calculated from the bug line number reported by the software engineering tool, if applicable. If the closest action is not a delete, then our approach take the location takes the location of that action. Otherwise, if the line was removed our algorithm searches for the closest sibling node or if none exists then the location of the parent. Additionally, the line of the fix had to be converted to the equivalent position in the pull request diff file displaying the changes between file versions, or number of lines below the "@@" header\footnote{https://developer.github.com/v3/pulls/comments/\#create-a-comment}, before moving on to the next phase.

\pseudosubsection{Code Review}

Code reviews from co-workers are often standard practice in software development~\cite{CBirdCodeReviewingTrenches}. Pull requests are the primary method of development contributions and code reviews on Github~\cite{PullRequestReview}. Our approach simulates peer reviews by making recommendations for static analysis tools as a comment to the pull request. \tool~automatically runs software engineering tools and analyzes the code changes, providing feedback to developers based on their changes based on the output from the tool. Github provides functionality for making comments at specific lines of code in a pull request, and \tool~recommends the tool as a comment at the fix location line from the previous step. Figure 1 presents an example recommendation from our system on a pull request. %TODO: Add screenshot 

To increase the likelihood of tool adoption, our system makes recommendations if the tool reports other instances of the fixed error in the latest version of the code. In the comment, \tool~automatically adds direct links to at most two separate locations of the same defect. Our hope is that this encourages developers to use software engineering toolsin their work to fix the similar errors and prevent additional bugs. Additionally, our system uses language similar to a colleage in recommendations. For instance, \tool~uses ``Good job!'' to compliment developers for fixing an error in their work~\cite{?}.

\section{Methodology}

Our study evaluates the effectiveness of \tool~by analyzing how often our system recommends software engineering tools and how developers respond to recommendations from our system.

\subsection{Receptiveness}

Our goal is for \tool~to increase awareness of software engineering tools. For our initial evaluation in this paper, we target software engineers' desire to produce mistake-free code by automatically recommending \textsc{Error Prone}. \textsc{Error Prone} is a static analysis tool that uses a suite of defined bug patterns to detect errors in Java code. Static analysis tools like \textsc{Error Prone} are useful for debugging and preventing errors in source code for applications, however they are often underutilized by software engineers~\cite{Johnson2013Why}. 

To choose a familiar audience, our approach makes recommendations on Github pull requests, or proposed code changes submitted by users. Developers should be knowledgeable and familiar with the changes they propose, as well as the code base to which they are contributing. \tool~suggests \textsc{Error Prone} when a reported error is fixed by a developer in a pull request but exists elsewhere in the code. 

\subsection{Projects}

We used real-world open source software applications to evaluate \tool. To choose study projects from the millions of GitHub repositories, we selected projects that met the following criteria:

\begin{itemize}
\item primariy written in Java,
\item ranked a top 1000 monthly forked repositoy in the last 10 years,
\item build using Maven, and
\item do not already utilize \textsc{Error Prone}.
\end{itemize}

We chose the most forked repositories to sample the most popular GitHub projects in terms of developer activity. Additionally, GitHub suggests forking repositories as a best practice for making contributions and pull requests to projects\footnote{https://guides.github.com/activities/forking/}. \textsc{Error Prone} only analyzes Java code, so this evaluation was limited to Java projects.  Maven is a build automation and dependency management system for Java projects. \tool~requires Maven to automatically modify \textit{pom.xml} files by adding the \textsc{Error Prone} plug-in and running the static analysis tool. Using a GitHub search API with the above constraints, we identified 2003 projects to use for our evaluation.\footnote{\url{https://docs.google.com/spreadsheets/d/1MRmVPiUeDlYMa0UODzMmFbGeYBbahzdeEv1YSgqC1oo/edit?usp=sharing}}

\subsection{Study Design}

We designed our evaluation to gather quantitative and qualitative data addressing our research questions.  To analyze 2000+ repositories simultaneously, we used Ansible\footnote{https://www.ansible.com/} to generate Jenkins jobs with \tool~running on multiple virtual machines.

\pseudosubsection{RQ1}

To answer our first research question, we observed how often our approach makes recommendations on newly opened pull requests. In addition to the frequency of recommendations, we also tracked instances where \textsc{Error Prone} defects were removed but not reported as fixed according to our fix identification algorithm in Section III.B.2, the number of occurrences where errors were fixed but no other instances of that bug were found in the code, and the false positive rate.

Johnson and colleagues discovered one of the primary barriers to static analysis tool usage among software engineers is false positives in the output~\cite{Johnson2013Why}. To prevent unnecessary recommendations from our system, we manually reviewed all instances where \tool~reported a recommendation should be made. After inspecting each reported pull request, we determined whether it was an appropriate case for our system to make a recommendation to developers. If so, we proceeded to post the comment recommending \textsc{Error Prone} on the pull request. Otherwise, we noted the instance of a false positive in our approach and did not make a recommendation.

\pseudosubsection{RQ2}

To gather data on the usefulness of our system, we sent a follow-up survey to developers. Survey participants were users who received a recommendation from \tool~on their pull request. We asked developers about their awareness of \textsc{Error Prone} and how likely they are to use the tool in the future. The survey also included a free-response section to provide an opportunity for participants to add comments on the usefulness of the recommendation.

Developers voluntarily consented to complete the survey and provide feedback on our system. To ensure developers answered honestly, we notified respondents that their answers will be used for research purposes. Previous research has shown that survey participants are more motivated to answer truthfullly if they know they are contributing to research~\cite{Krosnick1991Research}.

\subsection{Data Analysis}

We analyzed the data collected in our study to determine effectiveness of our automated tool recommendation system.

\pseudosubsection{RQ1}

Effective tool recommendation systems should have ample opportunities to regularly make recommendations to users. To determine how often \tool~automatically recommends \textsc{Error Prone}, we observed the number of pull request comments made by our tool. Our evaluation lasted for [some period of time], and we calculated the amount of true positive recommendations during that time span to measure the recommendation rate for each GitHub repository used in our study. To calculate the false positive rate, we compared the number of unnecessary instances where our tool proposed a recommendation found by researchers to the total number of instances where a recommendation reported by \tool.

\pseudosubsection{RQ2}

For our second research question, we accumulated responses from developers in our follow-up survey to determine the usefulness of our system. We utilized a five-point Likert scale for participants to rank how knowledgeable they were about the existence of \textsc{Error Prone} before the recommendation and how likely they are to use \textsc{Error Prone} for future development tasks. An optional free response section was provided at the end for respondents to describe explain why or why not they found the recommendation useful. These responses were used to analyze developers' reactions to our automated recommendation. Finally, researchers analyzed and independently coded open-ended responses from participants to further analyze the effectiveness of our system based on developer feedback.

\section{Results}

\subsection{How often can we expect \tool~to make recommendations?}

Tons of recommendations... \\

No false positives...

\subsection{How useful are recommendations from \tool~to developers?}

Excellent responses from recommendees...\\

Something statistically significant...

\section{Discussion}

\subsection{Observations}

\subsubsection{Why Were There So Few Recommendations?}

Non-java changes, documentation, errors removed not fixed, error fixed everywhere in code...

\subsection{Implications}

Here's what our results say about improving tool recommendation systems...

\section{Limitations}

Internal\\

An external threat to the validity of our study is that we only observed open source projects hosted on Github in our evaluation. Our results may not generalize to closed source software projects and their developers. To minimize this, we selected popular real-world software applications on Github owned by organizations to avoid the use of personal development projects. Additionally, our recommendation system has limited generalizablility due to the fact we currently only assess recommendations for the Error Prone static analysis tool on Java projects that build with Maven. Future work will look to extend \tool~to include different types of tools, programming languages, and build systems.

\section{Future Work}

More programming languages (Python, C, C++ etc.)...\\

More types of tools to recommend (static analysis, security, debugging, etc.)... \\

More build systems (ant, gradle, TravisCI, bazel)...\\

\section{Conclusion}

\tool~is awesome

%\balance
%\section{Acknowledgments}

%Thanks to all of the student and professional data analysts who volunteered for this study.

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{paper}  
% You must have a proper ''.bib'' file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!

%% That's all folks!
\end{document}
