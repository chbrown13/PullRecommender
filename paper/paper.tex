\documentclass[sigconf,review,anonymous]{acmart}
\acmConference[ESEC/FSE 2018]{The 26th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}{4â€“9 November, 2018}{Lake Buena Vista, Florida, United States}
\usepackage{pdftexcmds}
%\usepackage[pdftex]{graphicx}
%\usepackage{multirow}
%\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{balance}
\usepackage{amssymb, marvosym}
\usepackage{threeparttable}
%\usepackage[bookmarks=false]{hyperref}
\usepackage{url}
\PassOptionsToPackage{hyphens}{url}
\hypersetup{colorlinks=true,breaklinks=true}
\usetikzlibrary{patterns,shapes,arrows}
\newcommand\tab[1][.5cm]{\hspace*{#1}}
\hyphenation{op-tical net-works semi-conduc-tor}
\raggedbottom
\newcommand{\tool}{\textsl{tool-recommender-bot}}

\begin{document}

% Copyright
\setcopyright{acmlicensed}

\title{\tool}

\author{Chris Brown and Emerson Murphy-Hill}
\affiliation{%
	\institution{North Carolina State University}
	\city{Raleigh}
	\state{NC}
}
\email{dcbrow10@ncsu.edu, emerson@csc.ncsu.edu}

\begin{abstract}
To increase software user productivity, toolsmiths create tools and features to automatically complete software tasks. However, these useful tools are often undiscovered or ignored by potential users, which can be problematic for industries, such as software engineering, that rely on worker efficiency and correctness. This paper introduces \tool, a novel automated recommendation system developed for researchers to make recommendations and increase awareness of their products. To help improve tool adoption among software engineers, \tool~integrates concepts from peer interactions and software engineering industry practices to suggest tools to developers, and our results suggest that this approach is effective in making tool recommendations to software developers on GitHub repositories.
\end{abstract}
\keywords{Software Engineering; Tool Recommendations; Tool Discovery;}

\maketitle

\section{Introduction}

Software is becoming increasingly prevalent in our society. To keep up with rising demands for new technology, software engineers are making efforts to emphasize \textit{software quality}. Throughout the software development life cycle, companies focus on metrics that impact both software producers and consumers such as functionality, reliability, usability, efficiency, maintainability, and portability~\cite{KitchenhamQualityTarget}. However, despite increased attention to improving software quality, buggy code remains a persisting and escalating problem. 

The 2017 Software Fail Watch by Tricentis discovered that software failures affected approximaltely half of the world's population impacting 3.7 billion users, and caused \$1.7 trillion in financial losses.\footnote{https://www.tricentis.com/software-fail-watch/} Additionally, the process of finding and fixing bugs in code, or debugging, continues to become increasingly time-consuming and costly.  A study by the National Institute of Standards and Technology reported that software engineers spend 70-80\% of their time debugging at work, and one error takes an average of 17.4 hours to debug~\cite{NIST}. Studies also show the cost of fixing an error becomes more expensive the longer it exists in source code~\cite{SEEconomics, SoftwareAssuranceSDLC}.

To improve code quality and prevent errors, researchers and toolsmiths have created software engineering tools to aid developers in their work. Prior work shows that tools for static analysis~\cite{UsingStaticAnalysis}, refactoring~\cite{Murphy-HillFitness}, security, and more are beneficial for improving code. These tools automatically perform a wide variety of software development tasks to save time and effort for developers. Additionally, the Software Engineering Body of Knowledge indicates the importance of development tools, noting they can be used to achieve ``desirable characteristics of software products''~\cite{SWEBOK}. Utilizing software enginering tools can reduce software errors and debugging costs while increasing developer productivity.

Althouth quality is a primary concern for developers and users and many software engineering tools exist to help improve code and prevent bugs, developers rarely use these tools~\cite{Johnson2013Why}. One of the primary barriers to tool adoption is the discoverability barrier, where users are unaware of a tool's existence~\cite{Murphy-HillScreencastingDiscovery}. This lack of awareness for software engineering tools can lead to poor code quality, inconvenienced users, and significant amounts of time and money spent fixing errors. While many automated approaches have been developed to increase knowledge of useful software tools and features, Murphy-Hill and colleagues found that learning about tools from colleagues during normal work activities, or \textit{peer interaction}, is the most effective mode of tool discovery~\cite{MurphyHill2011PeerInteraction}.

To help solve the tool discovery problem among software engineers, we developed \tool. Our system automatically recommends development tools by analyzing developer changes and integrating characteristics of peer interactions and software engineering practices. \tool~is designed for toolsmiths to increase awareness of their products by systematically running their tools and making customized recommendations to developers on GitHub\footnote{https://github.com}, the largest code hosting platform in the world~\cite{GousiosGitHub}. This paper seeks to answer the following research questions (RQs): \\

\noindent
\textbf{RQ1:} How applicable is \tool~to real-world software applications?  \\ 
\textbf{RQ2:} How useful are recommendations from \tool~to developers?  \\

To answer these questions, we evaluated \tool~recommending \textsc{Error Prone}\footnote{http://errorprone.info}, an open source Java static analysis tool. We examined the frequency of recommendations and how developers reacted to receiving suggestions from our system. This research makes the following contributions:

\begin{itemize}
 \item a novel recommender system for researchers to increase awareness of their software engineering tools, and
 \item an evaluation of \tool~ recommending \textsc{Error Prone} to GitHub developers
 \end{itemize}

\section{Related Work}
%TODO: Use related work from whatever conference committee
Our implementation of \tool~builds on previous research examining tool discovery, lack of tool adoption among software engineers, and automated recommendation systems.

Researchers have explored how humans learn about new tools. There are various methods to discover tools in software, and research suggests recommendations between peers is the most effective way to increase tool awareness. Murphy-Hill found that peer interactions were the most effective mode of tool discovery compared to tool encounters, tutorials, descriptions, social media, and discussion threads~\cite{MurphyHill2011PeerInteraction}~\cite{Murphy-Hill2015HowDoUsers}. Similarly, Welty discovered that software users sought help from colleagues more often than search engines and help menus~\cite{Welty2011Help}. To improve the effectiveness of recommendations from our system, we integrate qualities of peer interactions into \tool.

Previous work has also explored the tool discovery problem and barriers preventing users from adopting new tools, specifically in the software engineering industry. Researchers have created numerous tools to aid software engineers in their work, but these products are often ignored by developers~\cite{Ivanov2017Gaps}. Tilley and colleagues studied the challenges of adopting these research-of-the-shelf tools in industry~\cite{Tilley2003ROTS}. Johnson and colleagues reported reasons why software engineers don't use static analysis tools to help find and prevent bugs in their code~\cite{Johnson2013Why}. Xiao and colleagues examined barriers and social influences blocking developers from using security tools to detect and prevent vulnerabilities and malicious attacks~\cite{Xiao2014Security}. Our project aims to increase tool discovery and adoption among developers by automatically recommending useful software enginering tools.

There are numerous existing technical approaches created to solve the tool discovery problem. Fischer and colleagues found that systems requiring users to explicitly seek help, or passive help systems, are ineffective and active help systems are more useful for increasing tool awareness~\cite{Fischer1984ActiveHelpSystems}. Gordon and colleagues developed Codepourri, a system using crowdsourcing to make recommendations for Python code~\cite{Gordon2015Codepourri}. Linton and colleagues designed a recommender system called OWL (organization-wide learning) to disseminate tool knowledge using logs throughout a company~\cite{Linton2000OWL}. ToolBox was developed as a ``community sensitive help system'' by Maltzahn to recommend Unix commands~\cite{Maltzahn1995Toolbox}. Answer Garden helps users discover new tools based on common questions asked by colleagues~\cite{Ackerman1990AnswerGarden}. SpyGlass automatically recommends tools to help users navigate code~\cite{Viriyakattiyaporn2010Spyglass}. We developed \tool~to actively suggest useful programming tools and increase awareness for software engineers. Our approach differs from existing recommendation systems in the design and implementation of our tool.

\section{Tool}
\tool~aims to increase discovery and adoption of useful software engineering tools among developers. This section describes the design and implementation of our approach.

\subsection{Peer Interactions}
Previous research shows that recommendations between colleagues is the most effective way to increase tool discovery and adoption~\cite{MurphyHill2011PeerInteraction}. Murphy-Hill interviewed software engineers and found that user-to-user peer interactions occur less frequently but are much more effective that system-to-user solutions.~\cite{MurphyHill2011PeerInteraction}~\cite{Murphy-Hill2015HowDoUsers}. 

To better understand what makes peer interactions an effective mode of tool discovery, prior work by Brown and colleagues observed how software users recommend tools to each other while completing tasks. They found that \emph{receptiveness} is a significant factor in determining the effectiveness of a tool recommendation, while other characteristics such as politeness and persuasiveness do not significantly impact the outcome of a suggestion~\cite{vlhcc17}. We designed \tool~to integrate user receptivity into our approach for making tool recommendations to increase awareness of programming tools.

Previous work emphasizes the importance of user receptiveness. Fogg outlined best practicies for creating persuasive technology to change user behavior, and argued designers must choose a receptive audience~\cite{FoggPersuasive}. We define receptiveness using two criteria outlined by Fogg: 1) demonstrating desire and 2) familiarity with target behavior. Below we explain how we designed \tool~to recommend programming tools to software developers based on their desire and familiarity.

\subsubsection{Desire}

The primary desire of software users is to have enjoyable and problem-free experiences with software. Developers of these applications also have similar desires, to create high-quality and functioning programs for users. A 2002 study revealed that software engineers demonstrate this desire by spending the majority of the software development process and 70-80\% of their time testing and debugging code~\cite{NIST}. To aid developers in finding, fixing, and preventing various issues in code, many different types of tools have been created to help accomplish these tasks. However, despite the existence of effective tools for detecting errors, the number of bugs in software is increasing~\cite{HaveThingsChanged}. \tool~aims to increase tool discovery by recommending software engineering tools designed to promote developers' desires for improving software quality.

Our goal is for \tool~to increase awareness of software engineering tools. For our initial evaluation in this paper, we target software engineers' desire to produce mistake-free code by automatically recommending \textsc{Error Prone}. \textsc{Error Prone} is a static analysis tool that uses a suite of defined bug patterns to detect errors in Java code. Static analysis tools like \textsc{Error Prone} are useful for debugging and preventing errors in source code for applications, however they are often underutilized by software engineers~\cite{Johnson2013Why}. 

\subsubsection{Familiarity}

Choosing an audience familiar with the target behavior is also vital to increasing adoption. To increase use of helpful programming tools, our system focuses on making recommendations to software engineers within the context of the projects they develop. Scalabrino and colleagues claim code understandability is one of the most important factors for software development, maintenance, debugging, and testing~\cite{Scalabrino2017Understandability}. \tool~uses familiarity with source code to recommend tools to software engineers in code they modify.

To choose a familiar audience, our approach makes recommendations on proposed Github code changes submitted by users. Developers should be knowledgeable and familiar with the changes they propose, as well as the code base to which they are contributing. \tool~suggests \textsc{Error Prone} when a reported error is fixed by a developer in a pull request or commit, and the same bug exists elsewhere in the code. 

\subsection{Software Engineering}

\tool~incorporates key software engineering industry concepts to improve tool discovery among software developers.

\subsubsection{Continuous Integration}

Our system utilizes continuous integration to recommend useful tools beforecode changes are integrated into the main repository, or merged. \tool~is implemented as a plugin for Jenkins, ``the leading open source automation server'' for source code deployment and delivery.\footnote{https://jenkins.io/} The system uses Jenkins to clone Github repositories and periodically check for newly-opened pull requests and commits every 15 minutes. When a new code change is found, our system uses Jenkins to automatically run our approach to recommend useful software engineering tools.

To analyze the source code, we target projects that use the Maven~\footnote{https://maven.apache.org/} build automation and software management tool for Java applications. Our approach uses Maven to automatically handle dependencies and perform the static analysis when the project builds. We inject the desired software engineering tool as a Maven plugin to repository's \textit{pom.xml} project object model file to add it to the build process. \tool~then builds both the original version of the code before the proposed changes were made (base) and the changed version of the repository with the modifications implemented (head) to inspect differences. Using Maven allows \tool~to run on a large number of Java projects that use the popular build tool and also makes our approach extendable to recommend other tools implemented as Maven plugins in future work. 

\subsubsection{Fix Identification}

After analyzing the base and head versions of the code, our approach parses the build output of each version to determine if any reported errors were fixed in the code modifications. The software engineering tool identifies faults in the source code, and we take the tool's output and use it in an algorithm we developed to determine if GitHub user code changes fix a reported bug. Our technique uses the code differencing tool GumTree~\cite{GumTree} to identify actions (addition, delete, insert, move, and update) performed between altered code versions and parse the source code to convert the text into abstract syntax trees. 

To determine if an error was fixed, we take several things into consideration: First, our approach ignores errors that are reported as fixed but were not located in files modified by the developer. This ensures that the GitHub user will be familiar with the code change and error fix. Second, we ignore instances where only delete actions were detected between the base and head versions of a file. This avoids making recommendations in situations where bugs are removed but not fixed. For example, a refactoring task such as renaming a class is presented in the GitHub diff as removing code from the original file. This can eliminate bugs reported by Error Prone between code versions, but may also just move the error to another location in the source code. Similarly, we also ignore deprecated classes because the error reported was not fixed. These conditions help us minimize the number of false positives and prevent errant recommendations in our approach.

%\pseudosubsection{Fix Localizaton}
%TODO: Delete?
%When a fix is identified in the changed version of code, \tool~then aims to find the location of the fix in the head version. To find the modified line that fixed a bug, we use GumTree to parse the Java file and convert it into abstract syntax trees. We look for the action closest to the offset of the error node calculated from the bug line number reported by the software engineering tool, if applicable. If the closest action is not a delete, then our approach take the location takes the location of that action. Otherwise, if the line was removed our algorithm searches for the closest sibling node or if none exists then the location of the parent. Additionally, the line of the fix had to be converted to the equivalent position in the pull request or commit diff file displaying the changes between file versions, or number of lines below the "@@" header\footnote{https://developer.github.com/v3/pulls/comments/\#create-a-comment}, before moving on to the next phase.

\subsubsection{Code Review}

Code reviews from co-workers are often standard practice in software development~\cite{CBirdCodeReviewingTrenches}. Pull requests and commits are the primary methods of development contributions and code updates on GitHub~\cite{PullRequestReview}. Our approach simulates peer reviews by making recommendations for static analysis tools as a comment to the pull request or commit. \tool~automatically runs software engineering tools and analyzes the code changes, providing feedback to developers based on their changes based on the output from the tool. Github allows users to make comments on specific lines of code changed in pull requests and commits. \tool~leverages this functionality by recommending the tool as a comment at the fix location line from the previous step. Figure 1 presents an example recommendation from our system on a pull request. %TODO: Add screenshot 

To increase the likelihood of tool adoption, our system makes recommendations if the tool reports other instances of the fixed error in the latest version of the code. In the comment, \tool~automatically adds direct links to at most two separate locations of the same defect. Our hope is that this encourages developers to use software engineering toolsin their work to fix the similar errors and prevent additional bugs. Additionally, our system uses language similar to a colleage in recommendations. For instance, \tool~uses ``Good job!'' to compliment developers for fixing an error in their work~\cite{?}.

\section{Methodology}

Our study evaluates the effectiveness of \tool~by analyzing how often our system recommends software engineering tools and how developers respond to recommendations from our system.

\subsection{Projects}

We used real-world open source software applications to evaluate \tool. To choose study projects from the millions of GitHub repositories, we selected projects that met the following criteria:

\begin{itemize}
\item primariy written in Java,
\item build using Maven,
\item do not already use \textsc{Error Prone},
\item ranked among the top starred repositories
\end{itemize}

Our evaluation was limited to Java projects since \textsc{Error Prone} only analyzes Java code. To determine if a repository was a Maven project, we automatically checked if a Project Object Model (\textit{pom.xml}) configuration file was located in the home directory. We also checked to make sure that the pom.xml did not already contain the \textsc{Error Prone} plugin. Our approach targets projects that don't use \textsc{Error Prone} to recommend the tool to developers who are less likely to be knowledgeable about the tool. We chose the most starred repositories to study the most popular projects on GitHub. Stars are a social aspect of GitHub where users can indicate favorite projects and repositories of interest\footnote{https://help.github.com/articles/about-stars/}. 

To get the most popular repositories, we filtered GitHub projects by the amount of stars based on numbers in the Fibonnaci sequence. This allowed us to get a higher concentration of projects with lower number of stars while less projects will have a large number of stars. For filtering of repositories, we sorted the top 100 projects by the most recently updated. After using a GitHub search API to find projects that met the above criteria, we compiled a list of 789 code repositories. The projects used for our evaluation include a wide range of software applications providing a variety of services from large software companies such as Google and Apache to individual developers. A list of projects used for this study is publicly available online.\footnote{\url{https://gist.github.com/tool-recommender-bot/1769ccd148508beabcd273a731723860}}
%TODO: Maybe examples of projects?

\subsection{Study Design}

We designed our evaluation to gather quantitative and qualitative data addressing our research questions.  To analyze 700+ repositories simultaneously, we used Ansible\footnote{https://www.ansible.com/} to generate Jenkins jobs with \tool~running on multiple virtual machines.

\subsubsection{RQ1}

To answer our first research question, we observed how often our approach makes recommendations on commits and pull requests. In addition to the frequency of recommendations, we also tracked instances where \textsc{Error Prone} defects were removed but not reported as fixed according to our fix identification algorithm in Section III.B.2, the number of occurrences where errors were fixed but no other instances of that bug were found in the code, and the false positive rate.

Johnson and colleagues discovered one of the primary barriers to static analysis tool usage among software engineers is false positives in the output~\cite{Johnson2013Why}. To prevent unnecessary recommendations from our system, we manually reviewed all instances where \tool~reported a recommendation should be made. After inspecting each repository modification, \tool~determined whether it was an appropriate case to make a recommendation to the developer. If so, we proceeded to post the comment recommending \textsc{Error Prone} on the code change. Otherwise, we noted the instance of a false positive in our approach and did not make a recommendation. In situations where one commit or pull request provided multiple opportunities for a recommendation, the authors would examine the changes and select one of the errors reported as fixed to recommend \textsc{Error Prone} to the developer.

\subsubsection{RQ2}

To gather data on the usefulness of our system, we sent a follow-up survey to developers. Survey participants were users who received a recommendation from \tool~on their pull request or commit. We asked developers about their awareness of \textsc{Error Prone} and how likely they are to use the tool in the future. The survey also included a free-response section to provide an opportunity for participants to add comments on the usefulness of the recommendation.

Developers voluntarily consented to complete the survey and provide feedback on our system. To ensure developers answered honestly, we notified respondents that their answers will be used for research purposes. Previous research has shown that survey participants are more motivated to answer truthfullly if they know they are contributing to research~\cite{Krosnick1991Research}. 

To futher examine the effectiveness of \tool, we compared our approach to sending email recommendations to developers. To study this, we found similar instances of code fixes by GitHub users where our system would recommend a tool and, instead of making the recommendation with \tool~on the pull request or commit, send an email suggesting \textsc{Error Prone} to the developers. The emails also contained the recommendation feedback survey for developers, and we compared results to see how software engineers responded to receiving a recommendation by email vs. on GitHub from \tool. To send a recommendation via email, the developer must have an email address publicly available on the GitHub user profile. 

\subsection{Data Analysis}

We analyzed the data collected in our study to determine effectiveness of our automated tool recommendation system.

\subsubsection{RQ1}

Effective tool recommendation systems should have ample opportunities to regularly make recommendations to users. To determine how often \tool~automatically recommends \textsc{Error Prone}, we observed the total number of new pull requests and commits, and compared it to the amount recommendations made by our tool. We calculated the rate of true positive recommendations during the span of our study to measure the recommendation rate for each GitHub repository used in our evaluation. To calculate the false positive rate, we compared the number of unnecessary instances where our tool proposed a recommendation found by researchers to the total number of instances where a recommendation reported by \tool.

\subsubsection{RQ2}

For our second research question, we accumulated responses from developers in our follow-up survey presented in recommendations from \tool~and by email to determine the usefulness of our system. We utilized a five-point Likert scale for participants to rank how knowledgeable they were about the existence of \textsc{Error Prone} before the recommendation and how likely they are to use \textsc{Error Prone} for future development tasks. An optional free response section was provided at the end for respondents to describe explain why or why not they found the recommendation useful. These responses were used to analyze developers' reactions to our automated recommendation. Finally, researchers analyzed and independently coded open-ended responses from participants to further analyze the effectiveness of our approach based on feedback from software developers.

\section{Results}

\subsection{How often can we expect \tool~to make recommendations?}

Tons of recommendations... \\

No false positives...

\subsection{How useful are recommendations from \tool~to developers?}

Excellent responses from recommendees...\\

Something statistically significant...

\section{Discussion}

\subsection{Observations}

\subsubsection{Why Were There So Few Recommendations?}

Non-java changes, number of errors removed but not fixed, number of errors fixed without another instance in the code, manual inspection of pull requests and commits...

\subsection{Implications}

Here's what our results say about improving tool recommendation systems...

\section{Limitations}

An internal threat to the validity of this work is our use of code differencing to determine if developers intended to fix a bug in a commit or pull request. We cannot definitively determine the intentions of GitHub developers making changes to a repository, however two authors analyzed the code changes and came to an agreement on if the modified patch was a fix before making a recommendation to the developer. Additionally, although we used a Likert scale to measure if GitHub users who received a recommendation were likely to use \textsc{Error Prone} in the future, we did not measure if the tool was actually adopted by the developers for future tasks.

Our evaluation has limited generalizablility due to the fact we only assessed recommendations for the \textsc{Error Prone} static analysis tool. This restricted our study to evaluate Java projects and a specific set of errors that can be reported by the tool. We selected \textsc{Error Prone} because it is able to report a wide variety of errors based on bug patterns for Java code. Another external threat to the validity of this study is the projects selected for our evaluation. We only examined open source repositories on GitHub, and these results may not generalize to developers of closed source software or projects on other code hosting sites, such as BitBucket.\footnote{http://bitbucket.org} To minimize this threat, we evaluated \tool~on a large number of popular software applications on GitHub that provide many different services from a wide variety of software companies and developers. 

\section{Future Work}

To improve discovery of software engineering tools, we plan to increase the practicality of \tool~for researchers to implement our system with their projects. A next step is to extend our approach to work with multiple static analysis tools, such as Checkstyle\footnote{http://checkstyle.sourceforge.net/} for Java. This will allow us to have more opportunities to make recommendations to developers based on the output from different tools. Additionally, we plan to update our system to work with software engineering tools that integrate as plugins for different build systems such as Gradle\footnote{https://gradle.org/} and Travis CI\footnote{https://travis-ci.org/}, as opposed to just Maven projects. Future work will also extend \tool~to work with different types of tools to increase adoption and usage, for example Find Security Bugs\footnote{https://find-sec-bugs.github.io/} which is useful for finding security vulnerabilities in Java web applications. Finally, future work will consist of expanding \tool~to work with software engineering tools for different programming languages, such as the Pylint\footnote{https://www.pylint.org/} static analysis tool for Python, clang\footnote{https://clang.llvm.org/} compiler and static analyzer for C and C++, and more.



\section{Conclusion}

\tool~is awesome

%\balance
%\section{Acknowledgments}

%Thanks to all of the student and professional data analysts who volunteered for this study.

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{paper}  
% You must have a proper ''.bib'' file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!

%% That's all folks!
\end{document}
