\documentclass[conference]{IEEEtran}
\usepackage{pdftexcmds}
\usepackage[pdftex]{graphicx}
\usepackage{multirow}
\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{balance}
\usepackage{amssymb, marvosym}
\usepackage{threeparttable}
\usepackage[bookmarks=false]{hyperref}
\usepackage{url}
\PassOptionsToPackage{hyphens}{url}
\hypersetup{colorlinks=true,breaklinks=true}
\usetikzlibrary{patterns,shapes,arrows}
\newcommand\tab[1][.5cm]{\hspace*{#1}}
\hyphenation{op-tical net-works semi-conduc-tor}
\IEEEoverridecommandlockouts
\raggedbottom
\newcommand{\tool}{tool-recommender-bot}
\newcommand{\pseudosection}[1]{\vspace{2mm} \noindent {\bf #1}}
\newcommand{\pseudosubsection}[1]{\vspace{2mm} {\it #1}}

\begin{document}

% Copyright
%\setcopyright{acmlicensed}

\title{\tool}

\author{\IEEEauthorblockN{Chris Brown and Emerson Murphy-Hill}
\IEEEauthorblockA{Department of Computer Science\\
North Carolina State University\\
Raleigh, NC\\
Email: dcbrow10@ncsu.edu, emerson@csc.ncsu.edu}
}

\maketitle
\begin{abstract}
Recommendation systems were developed to improve the adoption of useful software tools and features designed to save time and effort in completing tasks that are often ignored by users. Previous research suggests that peer-to-peer recommendations is an effective mode of tool discovery, and the receptiveness of recommendees is a vital characteristic in determining the outcome of tool suggestions. To help increase awareness of useful tools, we developed and evaluated a new system \tool~designed to integrate aspects of peer interactions and user receptivity into automated tool recommendations for software developers of real-world applications. Our findings suggest that \tool~is awesome, cool, and very effective in improving tool discovery.
\end{abstract}

\begin{IEEEkeywords}
Software Engineering; Tool Recommendation; Tool Discovery; Open Source
\end{IEEEkeywords}

\section{Introduction}

Software has permeated nearly every area of our society. To keep up with the increasing demand for technology, software quality has become an increasingly important metric for software development teams. Processes such as peer code reviews, unit tests, continuous integration, test automation, and more have been implemented to ensure software maintains a high quality. 

Despite increased attention to the quality of code, the process of finding and fixing bugs in software, or debugging, is becoming more time-consuming and costly.  A study by the National Institute of Standards and Technology reported that software engineers spend 70-80\% of their time at work debugging, and on average it takes 17.4 hours to debug one error~\cite{NIST}. Additionally, these errors can cost companies millions of dollars.  For example, a recent bug found in the cryptocurrency Ethereum resulted in the loss of \$30 million.\footnote{https://qz.com/1034321/ethereum-hack-a-coding-error-led-to-30-million-in-ethereum-being-stolen/} Additionally, studies have shown the price of repairing these code failures becomes more expensive the longer the bug exists~\cite{SEEconomics}.
%TODO: find better example

To make software engineers more effective and efficient in their work, many types of tools have been created to automatically perform code analysis, refactoring, security checks, and more for programmers. One such type is static analysis tools. Static analysis tools can improve software quality by automatically inspecting code without running the program. These tools can be useful for finding code defects early in the development process before code is released. According to a report from IBM, the relative cost for fixing errors also drastically increases the longer the fault exists in the code base through the software development lifecycle~\cite{SoftwareAssuranceSDLC}. Previous studies also show that static analysis tools are effective in preventing bugs in code to save money for companies in addition to time and effort for developers~\cite{UsingStaticAnalysis}.

Althouth quality is a primary concern for software producers and consumers, developers often ignore these useful tools which help improve the quality of their code~\cite{Johnson2013Why}. There are many barriers to tool adoption, and one of the main reasons useful tools are ignored or underutilized is the discoverability barrier. This refers to when users are unaware of a tool's existence within software~\cite{Murphy-HillScreencastingDiscovery}. The tool discovery problem will continue to persist as applications become more ``bloated'' with features~\cite{SoftwareBloat}. This lack of awareness of static analysis tools can lead to significant amounts of wasted time and money in the software industry.

%Researchers have explored methods for increasing tool discovery among software users. Fischer suggests that some of the more popular methods for raising tool awareness, such as documentation and help menus, are ineffective for increasing adoption of useful tools and features~\cite{ActiveHelpSystems}. These \textit{passive help systems} require users to first recognize the system's existence, navigate to the items, and then manually search for help. However, users rarely seek help by discovering new functionality and will often just rely on their own previous knowledge~\cite{something}. 
%
%Active help systems are the preferred method of helping users discover new functionality in software. These systems automatically recommend useful tools to software users. However, there remains room for improvement among these system-to-user recommendation systems. For example, Clippy, an intelligent office assistant for Microsoft Office products, was able to automatically suggest tools and features to Microsoft software users. However, the system was found ineffective and eventually removed. Peer interactions and receptiveness are effective~\cite{vlhcc17}...

To solve the static analysis tool discovery problem, we developed a new recommender system called~\textit{\tool}. \tool~was designed to automatically recommend static analysis tools to software engineers. Previous research has shown that user-to-user recommendations are the most effective mode of increasing awareness of tools among software users~\cite{MurphyHill2011PeerInteraction}. We designed our system to make suggestions by integrating characteristics of peer interactions and concepts from the software engineering industry. To evaluate the effectiveness of our system, we studied the following research questions (RQs): \\

\noindent
\textbf{RQ1:} How often can we expect \tool~to make recommendations?  \\
\textbf{RQ2:} How useful are recommendations from \tool~to developers?  \\

To answer these questions, we evaluated our system on open source Java projects to observe how many tool suggestions would be made based on past changes to the code base. We also examined how software developers reacted to receiving tool recommendations from our system. The main research contribution of our work is introducing the design and evaluation of a new automated recommendation system \tool, a novel approach to improving the discovery of static analysis tools for software engineers by simulating tool recommendations made between peers.

\section{Related Work}

Our approach is based on previous research examining how users learn about and discover tools, the lack of tool adoption among software engineers, and existing automated tool recommendation systems.

Researchers have explored ways humans learn about new tools. Prior work in diffusion of innovations~\cite{Rogers2003Diffusion} and persuasion theory~\cite{Shen2012Persuasion} show that the way messages are communicated impacts reception. There are a numerous methods for discovering tools in software, however, research suggests that recommendations from peers, or \textit{peer interactions}, are the most effective. Murphy-Hill found that peer interactions were the most effective mode of tool discovery among software engineers compared to chance encounters, tutorials, descriptions, social media, or discussion threads~\cite{MurphyHill2011PeerInteraction}~\cite{Murphy-Hill2015HowDoUsers}. Similarly, Welty discovered that software users sought help from colleagues more than search engines and help menus~\cite{Welty2011Help}. Our work builds on the results of these studies by integrating qualities of effective peer-to-peer recommendations into an automated recommendation system.

Previous work has explored the tool adoption problem in software engineering and barriers preventing developers from adopting useful tools for important programming tasks. Researchers have created numerous tools to aid software engineers in their work, but these products are often ignored by developers~\cite{Ivanov2017Gaps}. Tilley and colleagues studied the challenges of adopting these research-of-the-shelf tools in industry~\cite{Tilley2003ROTS}. Johnson and colleagues reported reasons why software engineers don't use static analysis tools to help find and prevent bugs in their code~\cite{Johnson2013Why}. Xiao and colleagues examined barriers and social influences blocking developers from using security tools to detect and prevent vulnerabilities and malicious attacks~\cite{Xiao2014Security}. Our project aims to increase software engineer tool adoption by developing \tool~to automatically and effectively recommend useful tools to help complete development tasks.

Additionally, there are numerous existing technical approaches created to improve the tool discovery problem. Researchers have developed several active help systems using community involvement. Gordon and colleagues developed Codepourri, a system using crowdsourcing to collectively create Python tutorials~\cite{Gordon2015Codepourri}. Linton and colleagues designed a recommender system called OWL (organization-wide learning) to disseminate tool knowledge using logs throughout a company~\cite{Linton2000OWL}. ToolBox was developed as a ``community sensitive help system'' by Maltzahn to recommend Unix commands~\cite{Maltzahn1995Toolbox}. Answer Garden helps users discover new tools based on common questions asked by colleagues~\cite{Ackerman1990AnswerGarden}. SpyGlass automatically recommends tools to help users navigate code~\cite{Viriyakattiyaporn2010Spyglass}

Fischer and colleagues examined systems that require users to explicitly seek help from the software, or passive help systems~\cite{Fischer1984ActiveHelpSystems}. Fischer concluded technical passive recommendation systems, such as documentation and help menues, are ineffective and inefficient for users. Instead, active help systems are more useful for increasing tool awareness. We developed \tool~to actively suggest useful programming tools and increase awareness for software engineers. Our approach differs from existing recommendation systems in the design and implementation of our tool.

\section{Tool}
Our approach to improving tool discovery, \tool, aims to increase awareness and use of programming tools among software developers. This section describes the design and implementation of our system.

\subsection{Design}
Previous research shows that recommendations between peers is an effective way to increase tool discovery and adoption~\cite{MurphyHill2011PeerInteraction}. Many existing help systems simulate user-to-user recommendations to increase awareness of application tools and features. 

To better understand what makes peer interactions an effective mode of tool discovery, our prior work observed how colleagues recommend tools to each other while working on tasks. Our results found that \emph{receptiveness} is a significant factor in determining the effectiveness of a tool recommendation, while other characteristics, such as politeness and persuasiveness, do not significantly impact the outcome~\cite{vlhcc17}. We designed \tool~to integrate user receptivity into our approach for making tool recommendations to increase awareness of programming tools.

\pseudosection{Receptiveness}

Previous work emphasizes the importance of receptivity. Fogg outlined best practicies for creating persuasive technology to change user behavior, and argued designers must choose a receptive audience~\cite{FoggPersuasive}. Our prior work defined receptiveness using two criteria introduced by Fogg: 1) demonstrating a desire and 2) familiarity with the target behavior and technology. Below we explain how \tool~was designed to recommend programming tools to software developers based on their desire and familiarity.

\pseudosubsection{1. Desire}

The primary desire of software users is to have enjoyable and problem-free experiences with software. Developers of these applications also have similar desires, to create high-quality and functioning programs for users. A 2002 study revealed that software engineers demonstrate this desire by spending the majority of the software development process and 70-80\% of their time testing and debugging code~\cite{NIST}. To aid developers in finding, fixing, and preventing various issues in code, many different types of tools have been created to help accomplish these tasks. However, despite the existence of effective tools for detecting errors, the number of bugs in software is increasing~\cite{HaveThingsChanged}. We aim to increase awareness of these tools to improve software quality and developer productivity, ultimately meeting users' and developers' desire for less buggy software.

To target this desire of mistake-free code, our initial implementation of \tool~automatically recommends the tool \textsc{Error Prone}.\footnote{http://errorprone.info} \textsc{Error Prone} is a static analysis tool created by Google to check for errors in Java code based on a suite of bug patterns. Static analysis tools like \textsc{Error Prone} are useful for debugging and preventing errors in source code for applications, however they are often underutilized by software engineers~\cite{Johnson2013Why}. 

\pseudosubsection{2. Familiarity}

Choosing an audience familiar with the target behavior is also vital to increasing adoption. To increase use of helpful programming tools, such as static analysis tools, our system focuses on making recommendations to software engineers within the context of the projects they develop. Familiarity with source code is important for creating software applications. Scalabrino and colleagues claim code understandability is one of the most important factors for software development, maintenance, debugging, and testing~\cite{Scalabrino2017Understandability}.

To choose a familiar audience, our approach makes recommendations on Github\footnote{https://github.com}, a popular source code management and version control website that hosts millions of projects and serves millions of users. \tool~makes its recommendations on pull requests, or proposed changes to source code submitted by programmers. Developers making these changes should be knowledgeable about the changes they propose as well as the code base to which they are contributing. Our approach suggests \textsc{Error Prone} when a reported error is fixed by a developer in a pull requests but still exists elsewhere in the code to capitalize on their familiarity with the modifications and encourage the use of static analysis tools to find more bugs. 

\subsection{Implementation}

\tool~builds on four key concepts to automatically recommend tools to users and improve tool discovery based on our design goals for targeting developer receptivity.

\pseudosubsection{1. Continuous Integration}

Our system utilizes continuous integration to recommend useful tools before pull request changes are integrated into the main repository, or merged. \tool~is implemented as a plugin for Jenkins, ``the leading open source automation server'' for source code deployment and delivery.\footnote{https://jenkins.io/} The system uses Jenkins to clone Github repositories and periodically check for newly-opened pull requests every 15 minutes. When a new pull request is found, our system uses Jenkins to automatically run our approach to recommend \textsc{Error Prone}.

To analyze the source code, we target projects that use the Maven~\footnote{https://maven.apache.org/} build automation and software management tool for Java applications. Our approach uses Maven to automatically handle dependencies and perform the static analysis when the project builds. We inject \textsc{Error Prone} as a Maven plugin to repository's \textit{pom.xml} project object model file to add it to the build process. \tool~then builds both the original version of the code before the proposed changes were made (base) and the changed version of the repository with the pull request modifications implemented (head) to inspect differences. Using Maven allows \tool~to run on a large number of Java projects that use the popular build tool and also makes our approach extendable to recommend other tools implemented as Maven plugins in future work. 

\pseudosubsection{2. Fix Identification}

After analyzing the base and head versions of the code, our approach parses the build output of each version to determine if any reported errors were fixed in the pull request. \textsc{Error Prone} identifies faults found in the source code, and we developed an algorithm using that information to determine if changes made to the code in the head version fix the identified bug. Our technique uses the code differencing tool GumTree~\cite{GumTree} to identify actions (addition, delete, insert, move, and update) performed between pull request versions and parse the code to convert the text into abstract syntax trees. 

To determine if an error was fixed, we take several things into consideration: First, our approach ignores instances where only delete actions were detected between the base and head versions of a file. This avoids making recommendations in situations where bugs were removed but not necessarily fixed in refactoring tasks, such as deleting and moving code, renaming classes, etc. Second, we ignore occurrences of deprecated classes because, similarly, the error reported was not fixed but removed. Third, we do not consider error fixes that were made by changes to a different file because we want to make recommendations where the developer is familiar with the changes that occurred. These help us minimize the number of false positives and errant recommendations in our approach.

\pseudosubsection{3. Fix Localizaton}

When a fix is identified in the pull request, \tool~then aims to find the location of the fix in the head version. To find the modified line that fixed a bug, we use GumTree to parse the Java file and convert it into abstract syntax trees. We look for the action closest to the offset of the error node calculated from the bug line number reported by \textsc{Error Prone}. If the closest action is not a delete, then our approach take the location takes the location of that action. Otherwise, if the line was removed our algorithm searches for the closest sibling node or if none exists then the location of the parent. Additionally, the line of the fix had to be converted to the equivalent position in the pull request diff file displaying the changes between file versions, or number of lines below the "@@" header\footnote{https://developer.github.com/v3/pulls/comments/\#create-a-comment}, before moving on to the next phase.

\pseudosubsection{4. Code Review}

Code reviews from co-workers are often standard practice in software development. Pull requests are the primary method of code contributions and code reviews on Github~\cite{PullRequestReview}. Our approach simulates peer reviews by making recommendations for static analysis tools as a comment to the pull request. \tool~automatically runs \textsc{Error Prone} and analyzes the code, providing feedback to developers based on their changes and the output of the tool. Github provides functionality for making comments at specific lines of code in a pull request, and \tool~recommends \textsc{Error Prone} as a comment at the fix location line from the previous step. Additionally, our system  uses language similar to comments between co-workers in recommendations, such as using ``Good job!'' to compliment developers on their work~\cite{?}.

To further increase adoption and use of static analysis tools, we only make a recommendation if \textsc{Error Prone} reports other instances of the same error in the base version of the code. In the comment, \tool~automatically adds direct links to at most two separate locations where the defect that was fixed still exists in the code. We hope this encourages the developer receiving the recommendation to use \textsc{Error Prone} to fix similar errors and prevent more bugs from being merged into the master code base. Figure 1 presents an example recommendation from our system on a pull request review.
%TODO: Add screenshot

\section{Methodology}

Our evaluation gathers both quantitative and qualitative data to evaulate the effectiveness of \tool.

\subsection{Projects}

To evaluate the effectiveness of our recommendation system, we assessed \tool~on real-world open-source software applications. Github hosts millions of code repositories, and to narrow down projects for our evaluation we picked public repos that met the following criteria:

\begin{itemize}
\item primarily written in the Java programming language\footnote{https://java.com},
\item build with Maven containing a \textit{pom.xml} file in the top directory,
\item and had at least 10 pull requests submitted by developers in the last six months (180 days).
\end{itemize}

Using the GitHub Search Repositories\footnote{https://developer.github.com/v3/search/\#search-repositories} functionality through the jcabi GitHub Java API\footnote{http://github.jcabi.com/}, we were able to identify 106 projects that met the above criteria. A full list of the repositories used in our evaluation is available to view online.\footnote{link}
\subsection{Participants}

Are we collecting any demographic information on the developers?

\subsection{Study Design}

We designed our study to address each of our research questions and address the influence of our recommendation system on improving tool discovery.

\pseudosubsection{RQ1}

To gather data on how often \tool~makes recommendations, we counted the number of pull request comments made by our tool throughout the duration of our study. We wanted to determine how regularly our approach has opportunities to make recommendations to developers. For each project, we accumulated the number of recommendations made over [some period of time]. To run our evaluation on the 106 repositories simultaneously, we installed \tool~as Jenkins jobs on multiple virtual machines to analyze the code bases in parallel. 

\pseudosubsection{RQ2}

To gather data on the usefulness of our system, we sent a follow-up survey to Github users who received a recommendation from our tool on a pull request they submitted. In our survey, we asked developers if they had ever heard of \textsc{Error Prone} before our tool's recommendation and to name static analysis tools they normally use while programming, if any. We also asked Likert-scale questions for the perceived usefulness of our recommendation and how likely participants are to use \textsc{Error Prone} in the future. The survey concluded by providing participants an opportunity to add any comments or feedback on our system and the recommendation they received.

Developers voluntarily consented to participate in the survey and provide feedback on our recommendation. To ensure participants answered honestly, we notified them that their responses would be used for research. Research shows that survey respondents are more motivated to answer truthfullly if they know they are contributing to research~\cite{Krosnick1991Research}.

\subsection{Data Analysis}

We analyzed the frequency of recommendations and follow-up survey responses to determine the effectiveness of our approach.

\pseudosubsection{RQ1}

To answer our first research question, we calculated the rate at which \tool~makes recommendations on opened pull requests. 
In addition to recommendation frequency, we collected other data to evaluate our system. We determined the false-positive rate for recommendations made by our system. To track instances where \textsc{Error Prone} defects were removed between versions of code, we also report the number of errors fixed without another instance found in the code and the number of bugs removed but not fixed according to our fix identification algorithm in Section III.B.2.

To prevent unnecessary comments on pull requests from our tool, all potential recommendations were reviewed by the researchers before posting to Github. Johnson discovered that one of the main barriers for underuse of static analysis tool among software engineers is reporting false positives in the tool output~\cite{Johnson2013Why}. We reviewed each pull request to determine if a fix was actually made. If so, \tool~could proceed and post the comment recommending \textsc{Error Prone} on the pull request. Otherwise we made note of a false positive found by our approach.

\pseudosubsection{RQ2}

For our second research question, we gathered data by accumulating responses from developers in our survey.

\section{Results}

\subsection{How often can we expect \tool~to make recommendations?}

Tons of recommendations... \\

\subsection{How useful are recommendations from \tool~to developers?}

Excellent responses from recommendees...\\

Statistically significant data...

\section{Discussion}

\subsection{Observations}

\subsection{Implications}

Here's what our results say about ways to improve tool recommendation systems...

\section{Limitations}

Internal\\

An external threat to the validity of our study is that we only observed open source projects hosted on Github in our evaluation. Our results may not generalize to closed source software projects and their developers. To minimize this, we selected popular real-world software applications on Github owned by organizations to avoid the use of personal development projects. Additionally, our recommendation system has limited generalizablility due to the fact we currently only assess recommendations for the Error Prone static analysis tool on Java projects that build with Maven. Future work will look to extend \tool~to include different types of tools, programming languages, and build systems.

\section{Future Work}

More tools to recommend (static analysis, security, etc.) \\

More programming languages instead of just java...\\

More build systems (ant, gradle, TravisCI, bazel)...\\

\section{Conclusion}

\tool~is awesome

%\balance
%\section{Acknowledgments}

%Thanks to all of the student and professional data analysts who volunteered for this study.

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{paper}  
% You must have a proper ''.bib'' file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!

%% That's all folks!
\end{document}
