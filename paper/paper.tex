\documentclass[sigconf,review,anonymous]{acmart}
\acmConference[ESEC/FSE 2018]{The 26th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering}{4â€“9 November, 2018}{Lake Buena Vista, Florida, United States}
\usepackage{pdftexcmds}
%\usepackage[pdftex]{graphicx}
%\usepackage{multirow}
%\usepackage{pgfplots}
\usepackage{tikz}
\usepackage{balance}
\usepackage{amssymb, marvosym}
\usepackage{threeparttable}
%\usepackage[bookmarks=false]{hyperref}
\usepackage{url}
\PassOptionsToPackage{hyphens}{url}
\hypersetup{colorlinks=true,breaklinks=true}
\usetikzlibrary{patterns,shapes,arrows}
\newcommand\tab[1][.5cm]{\hspace*{#1}}
\hyphenation{op-tical net-works semi-conduc-tor}
\raggedbottom
\newcommand{\tool}{\textsl{tool-recommender-bot}}

\begin{document}

% Copyright
\setcopyright{acmlicensed}

\title{\tool}

\author{Chris Brown and Emerson Murphy-Hill}
\affiliation{%
	\institution{North Carolina State University}
	\city{Raleigh}
	\state{NC}
}
\email{dcbrow10@ncsu.edu, emerson@csc.ncsu.edu}

\begin{abstract}
To increase software user productivity, toolsmiths create tools and features to automatically complete software tasks. However, these useful tools are often undiscovered or ignored by potential users, which can be problematic for industries, such as software engineering, that rely on worker efficiency and correctness. This paper introduces \tool, a novel automated recommendation system developed for researchers to make recommendations and increase awareness of their products. To help improve tool adoption among software engineers, \tool~integrates concepts from peer interactions and software engineering industry practices to suggest tools to developers, and our results suggest that this approach is effective in making tool recommendations to software developers on GitHub repositories.
\end{abstract}
\keywords{Software Engineering; Tool Recommendations; Tool Discovery;}

\maketitle

\section{Introduction}

Software is becoming increasingly prevalent in our society. To keep up with rising demands for new technology, software engineers are making efforts to emphasize \textit{software quality}. Throughout the software development life cycle, companies focus on metrics that impact both software producers and consumers such as functionality, reliability, usability, efficiency, maintainability, and portability~\cite{KitchenhamQualityTarget}. However, despite increased attention to improving software quality, buggy code remains a persisting and escalating problem. 

The 2017 Software Fail Watch by Tricentis discovered that software failures affected approximaltely half of the world's population impacting 3.7 billion users, and caused \$1.7 trillion in financial losses.\footnote{https://www.tricentis.com/software-fail-watch/} Additionally, the process of finding and fixing bugs in code, or debugging, continues to become increasingly time-consuming and costly.  A study by the National Institute of Standards and Technology reported that software engineers spend 70-80\% of their time debugging at work, and one error takes an average of 17.4 hours to debug~\cite{NIST}. Studies also show the cost of fixing an error becomes more expensive the longer it exists in source code~\cite{SEEconomics, SoftwareAssuranceSDLC}.

To improve code quality and prevent errors, researchers and toolsmiths have created software engineering tools to aid developers in their work. Prior work shows that tools for static analysis~\cite{UsingStaticAnalysis}, refactoring~\cite{Murphy-HillFitness}, security, and more are beneficial for improving code. These tools automatically perform a wide variety of software development tasks to save time and effort for developers. Additionally, the Software Engineering Body of Knowledge indicates the importance of development tools, noting they can be used to achieve ``desirable characteristics of software products''~\cite{SWEBOK}. Utilizing software enginering tools can reduce software errors and debugging costs while increasing developer productivity.

Althouth quality is a primary concern for developers and users and many software engineering tools exist to help improve code and prevent bugs, developers rarely use these tools~\cite{Johnson2013Why}. One of the primary barriers to tool adoption is the discoverability barrier, where users are unaware of a tool's existence~\cite{Murphy-HillScreencastingDiscovery}. This lack of awareness for software engineering tools can lead to poor code quality, inconvenienced users, and significant amounts of time and money spent fixing errors. While many automated approaches have been developed to increase knowledge of useful software tools and features, Murphy-Hill and colleagues found that learning about tools from colleagues during normal work activities, or \textit{peer interaction}, is the most effective mode of tool discovery~\cite{MurphyHill2011PeerInteraction}.

To help solve the tool discovery problem among software engineers, we developed \tool. Our system automatically recommends development tools by analyzing developer changes and integrating characteristics of peer interactions and software engineering practices. \tool~is designed for toolsmiths to increase awareness of their products by systematically running their tools and making customized recommendations to developers on GitHub\footnote{https://github.com}, the largest code hosting platform in the world~\cite{GousiosGitHub}. This paper seeks to answer the following research questions (RQs): \\

\noindent
\textbf{RQ1:} How applicable is \tool~to real-world software applications?  \\ 
\textbf{RQ2:} How useful are recommendations from \tool~to developers?  \\

To answer these questions, we evaluated \tool~recommending \textsc{Error Prone}\footnote{http://errorprone.info}, an open source Java static analysis tool. We examined the frequency of recommendations and how developers reacted to receiving suggestions from our system. This research makes the following contributions:

\begin{itemize}
 \item a novel recommender system for researchers to increase awareness of their software engineering tools, and
 \item an evaluation of \tool~ recommending \textsc{Error Prone} to GitHub developers
 \end{itemize}

\section{Related Work}
%TODO: Use related work from whatever conference committee
Our implementation of \tool~builds on previous research examining tool discovery, lack of tool adoption among software engineers, and automated recommendation systems.

Researchers have explored how humans learn about new tools. There are various methods to discover tools in software, and research suggests recommendations between peers is the most effective way to increase tool awareness. Murphy-Hill found that peer interactions were the most effective mode of tool discovery compared to tool encounters, tutorials, descriptions, social media, and discussion threads~\cite{MurphyHill2011PeerInteraction}~\cite{Murphy-Hill2015HowDoUsers}. Similarly, Welty discovered that software users sought help from colleagues more often than search engines and help menus~\cite{Welty2011Help}. To improve the effectiveness of recommendations from our system, we integrate qualities of peer interactions into \tool.

Previous work has also explored the tool discovery problem and barriers preventing users from adopting new tools, specifically in the software engineering industry. Researchers have created numerous tools to aid software engineers in their work, but these products are often ignored by developers~\cite{Ivanov2017Gaps}. Tilley and colleagues studied the challenges of adopting these research-of-the-shelf tools in industry~\cite{Tilley2003ROTS}. Johnson and colleagues reported reasons why software engineers don't use static analysis tools to help find and prevent bugs in their code~\cite{Johnson2013Why}. Xiao and colleagues examined barriers and social influences blocking developers from using security tools to detect and prevent vulnerabilities and malicious attacks~\cite{Xiao2014Security}. Our project aims to increase tool discovery and adoption among developers by automatically recommending useful software enginering tools.

There are numerous existing technical approaches created to solve the tool discovery problem. Fischer and colleagues found that systems requiring users to explicitly seek help, or passive help systems, are ineffective and active help systems are more useful for increasing tool awareness~\cite{Fischer1984ActiveHelpSystems}. Gordon and colleagues developed Codepourri, a system using crowdsourcing to make recommendations for Python code~\cite{Gordon2015Codepourri}. Linton and colleagues designed a recommender system called OWL (organization-wide learning) to disseminate tool knowledge using logs throughout a company~\cite{Linton2000OWL}. ToolBox was developed as a ``community sensitive help system'' by Maltzahn to recommend Unix commands~\cite{Maltzahn1995Toolbox}. Answer Garden helps users discover new tools based on common questions asked by colleagues~\cite{Ackerman1990AnswerGarden}. SpyGlass automatically recommends tools to help users navigate code~\cite{Viriyakattiyaporn2010Spyglass}. We developed \tool~to actively suggest useful programming tools and increase awareness for software engineers. Our approach differs from existing recommendation systems in the design and implementation of our tool.

\section{Tool}
\tool~aims to increase discovery and adoption of useful software engineering tools among developers. This section describes the design and implementation of our system for making recommendations similar to those from a peer and incorporating software engineering industry practices.

\subsection{Peer Interactions}
Previous research shows that recommendations between peers is an effective way to increase tool discovery and adoption~\cite{MurphyHill2011PeerInteraction}. Many existing help systems simulate user-to-user recommendations to increase awareness of application tools and features. 

To better understand what makes peer interactions an effective mode of tool discovery, prior work by Brown and colleagues observed how software users recommend tools to each other while completing tasks. They found that \emph{receptiveness} is a significant factor in determining the effectiveness of a tool recommendation while other characteristics, such as politeness and persuasiveness, do not significantly impact recommendation outcome~\cite{vlhcc17}. We designed \tool~to integrate user receptivity into our approach for making tool recommendations to increase awareness of programming tools.

Previous work emphasizes the importance of receptiveness. Fogg outlined best practicies for creating persuasive technology to change user behavior, and argued designers must choose a receptive audience~\cite{FoggPersuasive}. We define receptiveness using two criteria outlined by Fogg: 1) demonstrating desire and 2) familiarity with target behavior. Below we explain how we designed \tool~to recommend programming tools to software developers based on their desire and familiarity.

\subsubsection{Desire}

The primary desire of software users is to have enjoyable and problem-free experiences with software. Developers of these applications also have similar desires, to create high-quality and functioning programs for users. A 2002 study revealed that software engineers demonstrate this desire by spending the majority of the software development process and 70-80\% of their time testing and debugging code~\cite{NIST}. To aid developers in finding, fixing, and preventing various issues in code, many different types of tools have been created to help accomplish these tasks. However, despite the existence of effective tools for detecting errors, the number of bugs in software is increasing~\cite{HaveThingsChanged}. We aim to increase tool discovery by recommending software engineering tools designed to improve software quality.

\subsubsection{Familiarity}

Choosing an audience familiar with the target behavior is also vital to increasing adoption. To increase use of helpful programming tools, such as static analysis tools, our system focuses on making recommendations to software engineers within the context of the projects they develop. Familiarity with source code is important for creating software applications. Scalabrino and colleagues claim code understandability is one of the most important factors for software development, maintenance, debugging, and testing~\cite{Scalabrino2017Understandability}.

\subsection{Software Engineering}

\tool~incorporates key software engineering industry concepts to improve tool discovery among software developers.

\subsubsection{Continuous Integration}

Our system utilizes continuous integration to recommend useful tools before pull request changes are integrated into the main repository, or merged. \tool~is implemented as a plugin for Jenkins, ``the leading open source automation server'' for source code deployment and delivery.\footnote{https://jenkins.io/} The system uses Jenkins to clone Github repositories and periodically check for newly-opened pull requests every 15 minutes. When a new pull request is found, our system uses Jenkins to automatically run our approach to recommend useful software engineering tools.

To analyze the source code, we target projects that use the Maven~\footnote{https://maven.apache.org/} build automation and software management tool for Java applications. Our approach uses Maven to automatically handle dependencies and perform the static analysis when the project builds. We inject the desired software engineering tool as a Maven plugin to repository's \textit{pom.xml} project object model file to add it to the build process. \tool~then builds both the original version of the code before the proposed changes were made (base) and the changed version of the repository with the pull request modifications implemented (head) to inspect differences. Using Maven allows \tool~to run on a large number of Java projects that use the popular build tool and also makes our approach extendable to recommend other tools implemented as Maven plugins in future work. 

\subsubsection{Fix Identification}

After analyzing the base and head versions of the code, our approach parses the build output of each version to determine if any reported errors were fixed in the pull request. The software engineering tool identifies faults in the source code, and we take the tool's output and use it in an algorithm we developed to determine if pull request changes fix a reported bug. Our technique uses the code differencing tool GumTree~\cite{GumTree} to identify actions (addition, delete, insert, move, and update) performed between pull request versions and parse the code to convert the text into abstract syntax trees. 

To determine if an error was fixed, we take several things into consideration: First, our approach ignores instances where only delete actions were detected between the base and head versions of a file. This avoids making recommendations in situations where bugs were removed but not necessarily fixed in refactoring tasks, such as deleting and moving code, renaming classes, etc. Second, we ignore occurrences of deprecated classes because, similarly, the error reported was not fixed but removed. Third, we do not consider error fixes that were made by changes to a different file because we want to make recommendations where the developer is familiar with the changes that occurred. These help us minimize the number of false positives and errant recommendations in our approach.

%\pseudosubsection{Fix Localizaton}
%TODO: Delete?
%When a fix is identified in the pull request, \tool~then aims to find the location of the fix in the head version. To find the modified line that fixed a bug, we use GumTree to parse the Java file and convert it into abstract syntax trees. We look for the action closest to the offset of the error node calculated from the bug line number reported by the software engineering tool, if applicable. If the closest action is not a delete, then our approach take the location takes the location of that action. Otherwise, if the line was removed our algorithm searches for the closest sibling node or if none exists then the location of the parent. Additionally, the line of the fix had to be converted to the equivalent position in the pull request diff file displaying the changes between file versions, or number of lines below the "@@" header\footnote{https://developer.github.com/v3/pulls/comments/\#create-a-comment}, before moving on to the next phase.

\subsubsection{Code Review}

Code reviews from co-workers are often standard practice in software development~\cite{CBirdCodeReviewingTrenches}. Pull requests are the primary method of development contributions and code reviews on Github~\cite{PullRequestReview}. Our approach simulates peer reviews by making recommendations for static analysis tools as a comment to the pull request. \tool~automatically runs software engineering tools and analyzes the code changes, providing feedback to developers based on their changes based on the output from the tool. Github provides functionality for making comments at specific lines of code in a pull request, and \tool~recommends the tool as a comment at the fix location line from the previous step. Figure 1 presents an example recommendation from our system on a pull request. %TODO: Add screenshot 

To increase the likelihood of tool adoption, our system makes recommendations if the tool reports other instances of the fixed error in the latest version of the code. In the comment, \tool~automatically adds direct links to at most two separate locations of the same defect. Our hope is that this encourages developers to use software engineering toolsin their work to fix the similar errors and prevent additional bugs. Additionally, our system uses language similar to a colleage in recommendations. For instance, \tool~uses ``Good job!'' to compliment developers for fixing an error in their work~\cite{?}.

\section{Methodology}

Our study evaluates the effectiveness of \tool~by analyzing how often our system recommends software engineering tools and how developers respond to recommendations from our system.

\subsection{Receptiveness}

Our goal is for \tool~to increase awareness of software engineering tools. For our initial evaluation in this paper, we target software engineers' desire to produce mistake-free code by automatically recommending \textsc{Error Prone}. \textsc{Error Prone} is a static analysis tool that uses a suite of defined bug patterns to detect errors in Java code. Static analysis tools like \textsc{Error Prone} are useful for debugging and preventing errors in source code for applications, however they are often underutilized by software engineers~\cite{Johnson2013Why}. 

To choose a familiar audience, our approach makes recommendations on Github pull requests, or proposed code changes submitted by users. Developers should be knowledgeable and familiar with the changes they propose, as well as the code base to which they are contributing. \tool~suggests \textsc{Error Prone} when a reported error is fixed by a developer in a pull request but exists elsewhere in the code. 

\subsection{Projects}

We used real-world open source software applications to evaluate \tool. To choose study projects from the millions of GitHub repositories, we selected projects that met the following criteria:

\begin{itemize}
\item primariy written in Java,
\item build using Maven,
\item do not already use \textsc{Error Prone},
\item have at least 100 stars, and
\item ranked among the top forked repositories
\end{itemize}

Our evaluation was limited to Java projects since \textsc{Error Prone} only analyzes Java code. \tool~requires Maven, a build automation and dependency management system for Java projects, to run tools by automatically modifying the Project Object Model (\textit{pom.xml}) configuration file and building projects. We chose the most forked and starred repositories to study the most popular projects on GitHub. We target projects that don't use \textsc{Error Prone} to increase awareness for developers who are not knowledgeable about the tool. Stars are a social aspect of GitHub where users can indicate favorite projects and repositories of interest\footnote{https://help.github.com/articles/about-stars/}. GitHub suggests forking repositories as a best practice for making contributions and pull requests to projects\footnote{https://help.github.com/articles/fork-a-repo/}. 

After compiling a list of repositories that met the above criteria, we selected the top 250 projects based on the creation date of their last 10 pull requests. We chose projects with more recent pull requests to provide more opportunities for \tool~to make recommendations. The evaluation repositories include a wide range of software applications with diversified functions and created by many different companies and developers. A list of projects used for this study is publicly available online.\footnote{\url{https://gist.github.com/tool-recommender-bot/1769ccd148508beabcd273a731723860}}
%TODO: Maybe examples of projects?

\subsection{Study Design}

We designed our evaluation to gather quantitative and qualitative data addressing our research questions.  To analyze 2000+ repositories simultaneously, we used Ansible\footnote{https://www.ansible.com/} to generate Jenkins jobs with \tool~running on multiple virtual machines.

\subsubsection{RQ1}

To answer our first research question, we observed how often our approach makes recommendations on newly opened pull requests. In addition to the frequency of recommendations, we also tracked instances where \textsc{Error Prone} defects were removed but not reported as fixed according to our fix identification algorithm in Section III.B.2, the number of occurrences where errors were fixed but no other instances of that bug were found in the code, and the false positive rate.

Johnson and colleagues discovered one of the primary barriers to static analysis tool usage among software engineers is false positives in the output~\cite{Johnson2013Why}. To prevent unnecessary recommendations from our system, we manually reviewed all instances where \tool~reported a recommendation should be made. After inspecting each reported pull request, we determined whether it was an appropriate case for our system to make a recommendation to developers. If so, we proceeded to post the comment recommending \textsc{Error Prone} on the pull request. Otherwise, we noted the instance of a false positive in our approach and did not make a recommendation.

\subsubsection{RQ2}

To gather data on the usefulness of our system, we sent a follow-up survey to developers. Survey participants were users who received a recommendation from \tool~on their pull request. We asked developers about their awareness of \textsc{Error Prone} and how likely they are to use the tool in the future. The survey also included a free-response section to provide an opportunity for participants to add comments on the usefulness of the recommendation.

Developers voluntarily consented to complete the survey and provide feedback on our system. To ensure developers answered honestly, we notified respondents that their answers will be used for research purposes. Previous research has shown that survey participants are more motivated to answer truthfullly if they know they are contributing to research~\cite{Krosnick1991Research}.

\subsection{Data Analysis}

We analyzed the data collected in our study to determine effectiveness of our automated tool recommendation system.

\subsubsection{RQ1}

Effective tool recommendation systems should have ample opportunities to regularly make recommendations to users. To determine how often \tool~automatically recommends \textsc{Error Prone}, we observed the number of pull request comments made by our tool. Our evaluation lasted for [some period of time], and we calculated the amount of true positive recommendations during that time span to measure the recommendation rate for each GitHub repository used in our study. To calculate the false positive rate, we compared the number of unnecessary instances where our tool proposed a recommendation found by researchers to the total number of instances where a recommendation reported by \tool.

\subsubsection{RQ2}

For our second research question, we accumulated responses from developers in our follow-up survey to determine the usefulness of our system. We utilized a five-point Likert scale for participants to rank how knowledgeable they were about the existence of \textsc{Error Prone} before the recommendation and how likely they are to use \textsc{Error Prone} for future development tasks. An optional free response section was provided at the end for respondents to describe explain why or why not they found the recommendation useful. These responses were used to analyze developers' reactions to our automated recommendation. Finally, researchers analyzed and independently coded open-ended responses from participants to further analyze the effectiveness of our system based on developer feedback.

\section{Results}

\subsection{How often can we expect \tool~to make recommendations?}

Tons of recommendations... \\

No false positives...

\subsection{How useful are recommendations from \tool~to developers?}

Excellent responses from recommendees...\\

Something statistically significant...

\section{Discussion}

\subsection{Observations}

\subsubsection{Why Were There So Few Recommendations?}

Non-java changes, documentation, errors removed not fixed, error fixed everywhere in code...

\subsection{Implications}

Here's what our results say about improving tool recommendation systems...

\section{Limitations}

Internal\\

An external threat to the validity of our study is that we only observed open source projects, and our results may not generalize to closed source software projects and their developers. To minimize this, we evaluated \tool~on popular GitHub software applications with a wide variety of goals. Additionally, our recommendation system has limited generalizablility due to the fact we currently only assess recommendations for the Error Prone static analysis tool on Java projects that build with Maven.

\section{Future Work}

To improve discovery of software engineering tools, we aim to increase the practicality and convenience of \tool~for researchers to implement our system with their projects. \tool~currently only works for development tools for Java code. This disregards useful programming tools that focus on other languages such as...

Additionally, our system requires projects that build with Maven. This restriction ignores helpful software engineering tools that implement different build systems, for example,...

\section{Conclusion}

\tool~is awesome

%\balance
%\section{Acknowledgments}

%Thanks to all of the student and professional data analysts who volunteered for this study.

% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.
\bibliographystyle{abbrv}
\bibliography{paper}  
% You must have a proper ''.bib'' file
%  and remember to run:
% latex bibtex latex latex
% to resolve all references
%
% ACM needs 'a single self-contained file'!

%% That's all folks!
\end{document}
